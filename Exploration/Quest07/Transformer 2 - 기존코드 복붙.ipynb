{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f6b47dc-f154-4083-9af7-1f19e7a5e877",
   "metadata": {},
   "source": [
    "# 한국어 데이터로 챗봇 만들기\n",
    "프로젝트 제출 루브릭\n",
    "| **학습 목표** | **평가 기준** |\n",
    "|----------------|----------------|\n",
    "| 한국어 전처리를 통해 학습 데이터셋을 구축하였다. | 공백과 특수문자 처리, 토크나이징, 병렬데이터 구축의 과정이 적절히 진행되었다. |\n",
    "| 트랜스포머 모델을 구현하여 한국어 챗봇 모델 학습을 정상적으로 진행하였다. | 구현한 트랜스포머 모델이 한국어 병렬 데이터 학습 시 안정적으로 수렴하였다. |\n",
    "| 한국어 입력문장에 대해 한국어로 답변하는 함수를 구현하였다. | 한국어 입력문장에 맥락에 맞는 한국어로 답변을 리턴하였다. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363a79cd-5fd1-4327-b57a-dfeafa5a8899",
   "metadata": {},
   "source": [
    "# Step 0. Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "23c8f3a8-4883-45e0-830a-b44e376fb236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.12/site-packages (0.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "32e3439a-9021-4b80-aef7-5a95c4559aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import sentencepiece as spm\n",
    "\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caee125c-3a04-44d5-b9d7-3dd01e4ae18b",
   "metadata": {},
   "source": [
    "# Step 1. 데이터 수집하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0b12deaf-d8d8-4d00-adb4-b4aed6585b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://github.com/songys/Chatbot_data/raw/master/ChatbotData.csv\n",
    "#!mv ChatbotData.csv data/chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab255b0-762b-4fdb-83f9-bd764382d674",
   "metadata": {},
   "source": [
    "데이터 자체가 이런식으로 표현되어 있다.\n",
    "\n",
    "| **Q** | **A** | **Label** |\n",
    "|----------------|----------------|----------------|\n",
    "|12시 땡!, |하루가 또 가네요., |0|\n",
    "\n",
    "---\n",
    "\n",
    "데이터의 Label은 일상다반사 0, 이별(부정) 1, 사랑(긍정) 2로 레이블링로 되어있다.\n",
    "\n",
    "--- \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ab26c0-f278-4194-8f9a-e4f0981a3844",
   "metadata": {},
   "source": [
    "# Step 2. 데이터 전처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cbac6b56-bbfd-49b0-9c60-e4c250dc5884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11818</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>티가 나니까 눈치가 보이는 거죠!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11819</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>훔쳐보는 거 티나나봐요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11820</th>\n",
       "      <td>흑기사 해주는 짝남.</td>\n",
       "      <td>설렜겠어요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11821</th>\n",
       "      <td>힘든 연애 좋은 연애라는게 무슨 차이일까?</td>\n",
       "      <td>잘 헤어질 수 있는 사이 여부인 거 같아요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11822</th>\n",
       "      <td>힘들어서 결혼할까봐</td>\n",
       "      <td>도피성 결혼은 하지 않길 바라요.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11790 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Q                         A\n",
       "0                       12시 땡!                하루가 또 가네요.\n",
       "1                  1지망 학교 떨어졌어                 위로해 드립니다.\n",
       "2                 3박4일 놀러가고 싶다               여행은 언제나 좋죠.\n",
       "3              3박4일 정도 놀러가고 싶다               여행은 언제나 좋죠.\n",
       "4                      PPL 심하네                눈살이 찌푸려지죠.\n",
       "...                        ...                       ...\n",
       "11818           훔쳐보는 것도 눈치 보임.        티가 나니까 눈치가 보이는 거죠!\n",
       "11819           훔쳐보는 것도 눈치 보임.             훔쳐보는 거 티나나봐요.\n",
       "11820              흑기사 해주는 짝남.                    설렜겠어요.\n",
       "11821  힘든 연애 좋은 연애라는게 무슨 차이일까?  잘 헤어질 수 있는 사이 여부인 거 같아요.\n",
       "11822               힘들어서 결혼할까봐        도피성 결혼은 하지 않길 바라요.\n",
       "\n",
       "[11790 rows x 2 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_dir = os.path.join(\"data/chatbot/\")\n",
    "data = pd.read_csv(extract_dir+\"ChatbotData.csv\")\n",
    "#data = data[data.label == 2]\n",
    "data = data.drop(columns='label')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a94aad0a-2b30-4b5e-909c-d4e36380bffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    # 괄호, 따옴표, 기타 기호 제거\n",
    "    sentence = re.sub(r\"[\\(\\)\\[\\]\\\"\\'…]\", \"\", sentence)\n",
    "    \n",
    "    # 구두점 앞뒤 공백 정리\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    \n",
    "    # 한글, 구두점, 공백, 숫자 남기기\n",
    "    sentence = re.sub(r\"[^가-힣0-9?.!,\\s]\", \"\", sentence)\n",
    "    \n",
    "    # 다중 공백 축소\n",
    "    sentence = re.sub(r\"\\s+\", \" \", sentence).strip()\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b5105741-7cd2-4ca1-bd9e-f2b00eb0d03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_korean_chatbot_data(df):\n",
    "    \"\"\"\n",
    "    한국어 Q-A 챗봇 데이터셋을 읽어 (질문, 답변) 쌍으로 구성하는 함수.\n",
    "\n",
    "    Args:\n",
    "        path (str): CSV 파일 경로 (예: 'chatbot_data.csv')\n",
    "        max_samples (int, optional): 최대 추출할 (질문, 답변) 쌍 수. 기본값 None → 전체 사용\n",
    "\n",
    "    Returns:\n",
    "        list[tuple[str, str]]: (질문, 답변) 쌍의 리스트\n",
    "    \"\"\"\n",
    "    # 전처리 및 (질문, 답변) 쌍 구성\n",
    "    pairs = []\n",
    "    for _, row in df.iterrows():\n",
    "        q = preprocess_sentence(str(row['Q']))\n",
    "        a = preprocess_sentence(str(row['A']))\n",
    "\n",
    "        if q and a:\n",
    "            pairs.append((q, a))\n",
    "\n",
    "\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9ed27c16-be16-4c40-8a7a-8b9e2360c713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 수 : 11790\n"
     ]
    }
   ],
   "source": [
    "pairs = read_korean_chatbot_data(data)\n",
    "print('전체 샘플 수 :', len(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5975c1b-1573-4395-892b-09f4c73ad9c9",
   "metadata": {},
   "source": [
    "# Step 3. SentencePiece 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c6629654-30e3-47ef-a927-20f54491d37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_file = extract_dir+\"corpus.txt\"\n",
    "with open(corpus_file, 'w', encoding='utf-8') as f:\n",
    "    for q, a in pairs:\n",
    "        f.write(q + \"\\n\")\n",
    "        f.write(a + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "110240cc-a19b-4c10-a77c-3c58113a7a66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: data/chatbot/corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: data/chatbot/spm_korean\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 1200\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 999999\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(355) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(186) LOG(INFO) Loading corpus: data/chatbot/corpus.txt\n",
      "trainer_interface.cc(411) LOG(INFO) Loaded all 23580 sentences\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(432) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(541) LOG(INFO) all chars count=368877\n",
      "trainer_interface.cc(552) LOG(INFO) Done: 99.9504% characters are covered.\n",
      "trainer_interface.cc(562) LOG(INFO) Alphabet size=1052\n",
      "trainer_interface.cc(563) LOG(INFO) Final character coverage=0.999504\n",
      "trainer_interface.cc(594) LOG(INFO) Done! preprocessed 23580 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=177339\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 18237 seed sentencepieces\n",
      "trainer_interface.cc(600) LOG(INFO) Tokenizing input sentences with whitespace: 23580\n",
      "trainer_interface.cc(611) LOG(INFO) Done! 20591\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 20591 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=10724 obj=11.1038 num_tokens=40379 num_tokens/piece=3.76529\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=9296 obj=10.2605 num_tokens=40487 num_tokens/piece=4.35531\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=6972 obj=10.5554 num_tokens=43418 num_tokens/piece=6.22748\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=6971 obj=10.502 num_tokens=43453 num_tokens/piece=6.2334\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=5228 obj=10.9671 num_tokens=47231 num_tokens/piece=9.03424\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=5228 obj=10.8996 num_tokens=47233 num_tokens/piece=9.03462\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3921 obj=11.5084 num_tokens=51324 num_tokens/piece=13.0895\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3921 obj=11.427 num_tokens=51431 num_tokens/piece=13.1168\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2940 obj=12.0738 num_tokens=55805 num_tokens/piece=18.9813\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2940 obj=11.9804 num_tokens=55933 num_tokens/piece=19.0248\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2205 obj=13.012 num_tokens=60677 num_tokens/piece=27.5179\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2205 obj=12.8869 num_tokens=60799 num_tokens/piece=27.5732\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1653 obj=13.7431 num_tokens=66437 num_tokens/piece=40.1918\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1653 obj=13.5877 num_tokens=66601 num_tokens/piece=40.291\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1320 obj=14.4806 num_tokens=71717 num_tokens/piece=54.3311\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1320 obj=14.3283 num_tokens=71717 num_tokens/piece=54.3311\n",
      "trainer_interface.cc(689) LOG(INFO) Saving model: data/chatbot/spm_korean.model\n",
      "trainer_interface.cc(701) LOG(INFO) Saving vocabs: data/chatbot/spm_korean.vocab\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.Train(\n",
    "    input=corpus_file,                        # 학습할 말뭉치 파일 (한 줄에 한 문장)\n",
    "    model_prefix=extract_dir + \"spm_korean\",  # 출력 모델 파일 접두사\n",
    "    vocab_size=1200,                          # 토큰 개수 \n",
    "    character_coverage=0.9995,                # 한글 데이터는 0.9995 권장 (한자, 이모지 등 제외)\n",
    "    model_type=\"unigram\",                     # BPE보다 Unigram이 한국어에 자주 쓰임\n",
    "    max_sentence_length=999999,               # 문장 길이 제한 (충분히 크게 설정)\n",
    "    bos_id=1,                                 # 문장 시작 토큰 <s>\n",
    "    eos_id=2,                                 # 문장 종료 토큰 </s>\n",
    "    pad_id=0,                                 # 패딩 토큰\n",
    "    unk_id=3,                                 # 알 수 없는 토큰\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3670641f-021a-491c-8032-9f6055b02733",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(extract_dir+\"spm_korean.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "feb7f335-3279-4115-b064-0145fe4b1b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 후의 문장: 짝사랑만큼 고통스러운 건 없겠지 .\n",
      "Tokenized: ['▁짝사랑', '만', '큼', '▁', '고', '통', '스', '러', '운', '▁건', '▁', '없', '겠지', '▁.']\n",
      "Encoded: [273, 31, 1154, 4, 14, 251, 123, 116, 169, 137, 4, 1195, 156, 5]\n",
      "Decoded: 짝사랑만큼 고통스러운 건 없겠지 .\n"
     ]
    }
   ],
   "source": [
    "# 예제 문장\n",
    "sentence = \"짝사랑만큼 고통스러운 건 없겠지.\"\n",
    "\n",
    "sentence = preprocess_sentence(sentence)\n",
    "print(\"전처리 후의 문장:\", sentence)\n",
    "\n",
    "# 1. 토크나이징 (subword 단위로 분할)\n",
    "tokens = sp.encode(sentence, out_type=str)\n",
    "print(\"Tokenized:\", tokens)\n",
    "\n",
    "# 2. 인코딩 (서브워드를 정수 ID로 변환)\n",
    "encoded = sp.encode(sentence, out_type=int)\n",
    "print(\"Encoded:\", encoded)\n",
    "\n",
    "# 3. 디코딩 (정수 ID → 원본 문장 복원)\n",
    "decoded = sp.decode(encoded)\n",
    "print(\"Decoded:\", decoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8640a84c-1fdb-4274-9757-0f967a4b4d56",
   "metadata": {},
   "source": [
    "# Step 4. 모델 구성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2af65d8-7b12-459c-848b-3478333ce714",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Step 4-1. Model class 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4245b215-84dd-497c-a7ea-1ff0c2cc0bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, position, d_model):\n",
    "        \"\"\"\n",
    "        Positional Encoding 클래스\n",
    "\n",
    "        Args:\n",
    "            position (int): 문장의 최대 길이 (max sequence length)\n",
    "            d_model (int): 임베딩 벡터의 차원 (model dimension)\n",
    "\n",
    "        역할:\n",
    "            단어의 순서(위치) 정보를 임베딩 벡터에 더해줌으로써,\n",
    "            Transformer가 RNN처럼 순서 정보를 학습하지 않아도\n",
    "            위치 정보를 인코딩할 수 있도록 해줌.\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.position = position\n",
    "\n",
    "        # 위치별로 미리 계산된 사인/코사인 기반 위치 인코딩 행렬 생성\n",
    "        self.pos_encoding = self._build_pos_encoding(position, d_model)\n",
    "\n",
    "    def _get_angles(self, position, i, d_model):\n",
    "        \"\"\"\n",
    "        각 위치(pos)와 차원(i)에 대해 angle 값을 계산.\n",
    "\n",
    "        Args:\n",
    "            position (Tensor): [position, 1] 위치 인덱스\n",
    "            i (Tensor): [1, d_model] 차원 인덱스\n",
    "            d_model (int): 임베딩 차원\n",
    "\n",
    "        Returns:\n",
    "            Tensor: 위치-차원별 angle 값\n",
    "        \"\"\"\n",
    "        # (2 * (i // 2)) : 짝수/홀수 차원 구분용\n",
    "        return 1.0 / (10000.0 ** ((2.0 * (i // 2)) / d_model)) * position\n",
    "\n",
    "        # 사실 위처럼 지수계산보다는 log계산으로 푸는 것이 더 안정적이라 한다. \n",
    "        # angle_rates = torch.exp(- (2 * (i // 2)) * torch.log(torch.tensor(10000.0)) / d_model)\n",
    "        # return position * angle_rates.unsqueeze(0)\n",
    "\n",
    "    def _build_pos_encoding(self, position, d_model):\n",
    "        \"\"\"\n",
    "        전체 위치 인코딩 행렬을 생성.\n",
    "\n",
    "        Returns:\n",
    "            pos_encoding: [1, position, d_model]\n",
    "        \"\"\"\n",
    "        # 각 위치에 대해 0 ~ position-1까지의 인덱스 생성 → shape: [position, 1]\n",
    "        pos = torch.arange(position, dtype=torch.float32).unsqueeze(1)\n",
    "        # 각 차원에 대해 0 ~ d_model-1까지의 인덱스 생성 → shape: [1, d_model]\n",
    "        i = torch.arange(d_model, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        # 각 위치와 차원에 대한 각도(angle) 계산\n",
    "        angle_rads = self._get_angles(pos, i, d_model)\n",
    "\n",
    "        # 짝수 인덱스(0, 2, 4, ...)에는 sin, 홀수 인덱스(1, 3, 5, ...)에는 cos 적용\n",
    "        sines = torch.sin(angle_rads[:, 0::2])\n",
    "        cosines = torch.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        # 최종 위치 인코딩 행렬 초기화\n",
    "        pos_encoding = torch.zeros(position, d_model)\n",
    "        pos_encoding[:, 0::2] = sines  # 짝수 차원 → sin 값\n",
    "        pos_encoding[:, 1::2] = cosines  # 홀수 차원 → cos 값\n",
    "\n",
    "        # 배치 차원 추가: shape [1, position, d_model]\n",
    "        pos_encoding = pos_encoding.unsqueeze(0)\n",
    "        return pos_encoding\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        입력 임베딩(x)에 위치 인코딩을 더함.\n",
    "        Args:\n",
    "            x (Tensor): [batch_size, seq_len, d_model]\n",
    "        Returns:\n",
    "            Tensor: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        # 입력 길이(seq_len)에 맞는 부분만 잘라서 더함\n",
    "        return x + self.pos_encoding[:, :x.size(1), :].to(x.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "47a2a558-72c3-40d6-8129-3b7c64ff4c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention\n",
    "    -------------------------------------------------------\n",
    "    query: (batch_size, heads, seq_len_q, depth)\n",
    "    key:   (batch_size, heads, seq_len_k, depth)\n",
    "    value: (batch_size, heads, seq_len_v, depth)\n",
    "    mask:  (optional) attention mask tensor\n",
    "    return: (output, attention_weights)\n",
    "    -------------------------------------------------------\n",
    "    핵심 수식:\n",
    "        Attention(Q, K, V) = softmax( (QK^T) / sqrt(d_k) ) * V\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Query와 Key의 내적(dot-product)을 통해 유사도(score) 계산\n",
    "    #    - key.transpose(-1, -2): 마지막 두 차원을 전치하여 (depth, seq_len_k)\n",
    "    #    - 결과 shape: (batch_size, heads, seq_len_q, seq_len_k)\n",
    "    matmul_qk = torch.matmul(query, key.transpose(-1, -2))\n",
    "\n",
    "    # 2) Key 벡터의 차원(depth)에 따라 스케일링 (정규화)\n",
    "    #    - 큰 값으로 인한 softmax gradient vanishing 방지\n",
    "    #    - sqrt(depth)로 나눠줌\n",
    "    depth = key.size(-1)\n",
    "    logits = matmul_qk / math.sqrt(depth)\n",
    "\n",
    "    # 3) 마스크(mask)가 주어진 경우 적용\n",
    "    #    - 패딩 토큰 또는 미래 토큰(Decoder의 causal mask) 무시용\n",
    "    #    - 매우 작은 값(-1e9)을 더해 softmax에서 해당 위치의 확률을 0으로 만듦\n",
    "    if mask is not None:\n",
    "        logits = logits + (mask * -1e9)\n",
    "\n",
    "    # 4) Softmax를 통해 attention weight 계산\n",
    "    #    - 각 query에 대해 모든 key의 가중치 분포 생성\n",
    "    #    - dim=-1: seq_len_k 차원(즉, key 차원)에 대해 정규화\n",
    "    attention_weights = F.softmax(logits, dim=-1)\n",
    "\n",
    "    # 5) attention weight를 value에 곱해 weighted sum 계산\n",
    "    #    - 결과: (batch_size, heads, seq_len_q, depth)\n",
    "    output = torch.matmul(attention_weights, value)\n",
    "\n",
    "    # output: context vector, attention_weights: 각 token 간 주의 분포\n",
    "    return output, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "94d13111-c972-4946-97e4-d4f5e4bc716d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads  # 병렬적으로 나눠 계산할 헤드(head)의 개수\n",
    "        self.d_model = d_model      # 입력/출력의 전체 차원 (embedding dimension)\n",
    "\n",
    "        # d_model은 num_heads로 정확히 나누어떨어져야 함\n",
    "        # (예: d_model=512, num_heads=8 → head당 depth=64)\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        # 각 head에서 사용할 벡터의 차원\n",
    "        self.depth = d_model // num_heads\n",
    "\n",
    "        # Q, K, V를 생성하기 위한 선형 변환 (각각 d_model → d_model)\n",
    "        # 학습 가능한 가중치 행렬을 통해 입력을 head 차원으로 투영\n",
    "        self.query_dense = nn.Linear(d_model, d_model)\n",
    "        self.key_dense = nn.Linear(d_model, d_model)\n",
    "        self.value_dense = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # 모든 head를 다시 결합한 후, 최종 출력 차원 복원용 선형 변환\n",
    "        self.out_dense = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"\n",
    "        입력 x를 num_heads 개로 분리하는 함수\n",
    "        ---------------------------------------------------\n",
    "        x: (batch_size, seq_len, d_model)\n",
    "        반환: (batch_size, num_heads, seq_len, depth)\n",
    "        ---------------------------------------------------\n",
    "        - d_model을 num_heads로 나누어 각 head가 처리할 부분 벡터로 분할\n",
    "        - 이후 permute를 통해 head 차원을 앞으로 이동\n",
    "        \"\"\"\n",
    "        # (batch_size, seq_len, d_model) → (batch_size, seq_len, num_heads, depth)\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.depth)\n",
    "\n",
    "        # (batch_size, num_heads, seq_len, depth) 형태로 차원 재배치\n",
    "        x = x.permute(0, 2, 1, 3)\n",
    "        return x\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Multi-Head Attention의 순전파(forward) 과정\n",
    "        ---------------------------------------------------\n",
    "        query, key, value: (batch_size, seq_len, d_model)\n",
    "        mask: (optional) Attention mask\n",
    "        ---------------------------------------------------\n",
    "        출력: (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # 1) 입력 벡터에 각각 Linear layer 적용 → Q, K, V 생성\n",
    "        #    shape: (batch_size, seq_len, d_model)\n",
    "        query = self.query_dense(query)\n",
    "        key = self.key_dense(key)\n",
    "        value = self.value_dense(value)\n",
    "\n",
    "        # 2) Head별로 분리 (num_heads, depth 구조로 변환)\n",
    "        #    shape: (batch_size, num_heads, seq_len, depth)\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        # 3) 각 head별로 Scaled Dot-Product Attention 수행\n",
    "        #    반환값: (batch_size, num_heads, seq_len, depth)\n",
    "        scaled_attention, _ = scaled_dot_product_attention(query, key, value, mask)\n",
    "\n",
    "        # 4) 모든 head의 결과를 다시 결합하기 위해 차원 순서 재정렬\n",
    "        #    (batch_size, num_heads, seq_len, depth)\n",
    "        #      → (batch_size, seq_len, num_heads, depth)\n",
    "        scaled_attention = scaled_attention.permute(0, 2, 1, 3).contiguous()\n",
    "\n",
    "        # 5) num_heads와 depth를 결합해 원래 d_model 차원으로 복원\n",
    "        #    shape: (batch_size, seq_len, d_model)\n",
    "        concat_attention = scaled_attention.view(batch_size, -1, self.d_model)\n",
    "\n",
    "        # 6) 최종 선형 변환을 통해 head 통합 결과를 출력 차원으로 투영\n",
    "        #    (batch_size, seq_len, d_model)\n",
    "        output = self.out_dense(concat_attention)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "de532363-f9d3-429d-94d0-412a9523fcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(x):\n",
    "    # x == 0 위치를 찾아 float형 1로 변환\n",
    "    mask = (x == 0).float()\n",
    "    # (batch_size, seq_len) -> (batch_size, 1, 1, seq_len)\n",
    "    mask = mask.unsqueeze(1).unsqueeze(2)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1e8bf248-23a2-4562-bccb-0ced1075d7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(x):\n",
    "    seq_len = x.size(1)\n",
    "\n",
    "    # 1) Look-ahead 마스크 생성\n",
    "    # torch.ones((seq_len, seq_len)) → 1로 채워진 정방행렬 생성\n",
    "    # torch.tril() → 하삼각(자기 자신 포함) 부분만 남기고 나머지를 0으로 만듦\n",
    "    # 1 - tril(...) → 상삼각(즉, 미래 토큰 위치)이 1, 나머지는 0\n",
    "    # => Decoder가 아직 보지 않은 미래 단어를 참고하지 않도록 차단\n",
    "    look_ahead_mask = 1 - torch.tril(torch.ones((seq_len, seq_len)))\n",
    "\n",
    "    # 2) 입력 x에서 패딩 위치(0인 부분)를 찾아 패딩 마스크 생성\n",
    "    # shape: (batch_size, 1, 1, seq_len)\n",
    "    padding_mask = create_padding_mask(x)\n",
    "\n",
    "    # 3) Look-ahead 마스크 차원 확장\n",
    "    # (seq_len, seq_len) → (1, seq_len, seq_len) → (1, 1, seq_len, seq_len)\n",
    "    # => Attention 연산 시 브로드캐스팅이 가능하도록 형태 맞춤\n",
    "    look_ahead_mask = look_ahead_mask.unsqueeze(0).unsqueeze(1)\n",
    "    look_ahead_mask = look_ahead_mask.to(x.device)\n",
    "\n",
    "    # 4) Look-ahead 마스크와 패딩 마스크를 결합\n",
    "    # torch.max()를 사용하여 둘 중 하나라도 1인 위치는 모두 마스킹 처리\n",
    "    # 최종 shape: (batch_size, 1, seq_len, seq_len)\n",
    "    combined_mask = torch.max(look_ahead_mask, padding_mask)\n",
    "    \n",
    "    return combined_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e826f523-fb66-43cc-b906-45b4e3f6110e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, ff_dim, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        # (1) 멀티-헤드 어텐션 (Self-Attention)\n",
    "        # 입력 문장 내 단어들이 서로 어떤 관계를 맺고 있는지 학습\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=1e-6)  # 잔차 연결 후 정규화\n",
    "\n",
    "        # (2) 포지션별 피드포워드 네트워크 (Feed Forward Network)\n",
    "        # 각 위치의 특징을 비선형 변환을 통해 확장\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, ff_dim),  # 차원 확장\n",
    "            nn.ReLU(),                   # 비선형 활성화\n",
    "            nn.Linear(ff_dim, d_model)   # 원래 차원으로 복귀\n",
    "        )\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=1e-6)  # 두 번째 정규화\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: 입력 텐서 (batch_size, seq_len, d_model)\n",
    "            mask: 패딩 마스크 (선택적)\n",
    "\n",
    "        Returns:\n",
    "            out2: 인코더 레이어의 출력 (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "\n",
    "        # (1) 멀티-헤드 셀프 어텐션\n",
    "        # Query, Key, Value 모두 같은 입력 x를 사용\n",
    "        attn_output = self.mha(x, x, x, mask)         # (batch_size, seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output)      # 드롭아웃으로 과적합 방지\n",
    "        out1 = self.norm1(x + attn_output)            # 잔차 연결(residual) + LayerNorm\n",
    "\n",
    "        # (2) 포지션별 피드포워드 신경망\n",
    "        ffn_output = self.ffn(out1)                   # (batch_size, seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output)        # 드롭아웃 적용\n",
    "        out2 = self.norm2(out1 + ffn_output)          # 잔차 연결 + LayerNorm\n",
    "\n",
    "        return out2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4c7f6c65-ef35-42d2-8dc6-936c4d09e731",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 num_layers,\n",
    "                 ff_dim,\n",
    "                 d_model,\n",
    "                 num_heads,\n",
    "                 dropout=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # (1) 단어 임베딩 (Word Embedding)\n",
    "        # 입력 토큰 ID를 고정 크기 벡터(d_model 차원)로 변환\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "        # (2) 위치 임베딩 (Positional Encoding)\n",
    "        # 문장 내 단어 순서(위치) 정보를 추가해 순서 의존성 학습 가능하게 함\n",
    "        self.pos_encoding = PositionalEncoding(position=vocab_size, d_model=d_model)\n",
    "\n",
    "        # 드롭아웃: 학습 시 일부 뉴런 비활성화로 과적합 방지\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # (3) 인코더 블록(EncoderLayer)들을 num_layers 개만큼 쌓기\n",
    "        self.enc_layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, num_heads, ff_dim, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: 입력 토큰 시퀀스 (batch_size, seq_len)\n",
    "            mask: 패딩 마스크 (선택적)\n",
    "\n",
    "        Returns:\n",
    "            x: 인코더의 출력 벡터 (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "\n",
    "        # (1) 단어 임베딩 + 스케일링\n",
    "        # sqrt(d_model)로 스케일링해 학습 안정화 (Attention 계산 시 값이 너무 작아지는 것 방지)\n",
    "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
    "\n",
    "        # (2) 포지셔널 인코딩 추가 + 드롭아웃\n",
    "        # 위치 정보가 추가된 임베딩이 self-attention으로 입력됨\n",
    "        x = self.pos_encoding(x)                     # (batch_size, seq_len, d_model)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # (3) N개의 EncoderLayer를 순차적으로 통과\n",
    "        # 각 레이어에서 self-attention → feed-forward → 정규화 과정을 거침\n",
    "        for layer in self.enc_layers:\n",
    "            x = layer(x, mask)\n",
    "\n",
    "        # (4) 최종 인코더 출력\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8f532100-9d2f-493e-b545-3397cb3a8fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, ff_dim, dropout=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        # (1) 디코더 내부의 셀프 어텐션 (Masked Multi-Head Attention)\n",
    "        # => 이전 시점까지만 참조하도록 Look-ahead Mask 사용\n",
    "        self.self_mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "\n",
    "        # (2) 인코더-디코더 어텐션\n",
    "        # => 디코더가 인코더의 출력(컨텍스트)을 참고하도록 하는 Cross-Attention\n",
    "        self.encdec_mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "\n",
    "        # (3) 포지션별 피드포워드 네트워크 (Position-wise Feed Forward Network)\n",
    "        # => 각 위치의 피처를 독립적으로 비선형 변환\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, ff_dim),  # 확장\n",
    "            nn.ReLU(),                   # 비선형 활성화\n",
    "            nn.Linear(ff_dim, d_model)   # 축소 (원래 차원 복원)\n",
    "        )\n",
    "        self.norm3 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "\n",
    "        # 드롭아웃: 각 서브 레이어의 출력에 적용\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_outputs, look_ahead_mask=None, padding_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: 디코더 입력 (batch_size, target_seq_len, d_model)\n",
    "            enc_outputs: 인코더 출력 (batch_size, input_seq_len, d_model)\n",
    "            look_ahead_mask: 미래 단어를 가리는 마스크\n",
    "            padding_mask: 인코더 출력에서 패딩 위치를 가리는 마스크\n",
    "\n",
    "        Returns:\n",
    "            out3: 디코더 레이어의 출력 (batch_size, target_seq_len, d_model)\n",
    "        \"\"\"\n",
    "\n",
    "        # (1) Masked Self-Attention\n",
    "        # => 디코더가 다음 단어를 미리 보지 않도록 Look-ahead Mask 사용\n",
    "        self_attn_out = self.self_mha(x, x, x, mask=look_ahead_mask)\n",
    "        self_attn_out = self.dropout1(self_attn_out)\n",
    "        out1 = self.norm1(x + self_attn_out)  # 잔차 연결 + LayerNorm\n",
    "\n",
    "        # (2) Encoder-Decoder Attention (Cross-Attention)\n",
    "        # => 인코더 출력(enc_outputs)을 Key, Value로 사용해 입력 문맥 정보 활용\n",
    "        encdec_attn_out = self.encdec_mha(out1, enc_outputs, enc_outputs, mask=padding_mask)\n",
    "        encdec_attn_out = self.dropout2(encdec_attn_out)\n",
    "        out2 = self.norm2(out1 + encdec_attn_out)  # 잔차 연결 + LayerNorm\n",
    "\n",
    "        # (3) Position-wise Feed Forward Network\n",
    "        # => 각 시점별로 독립적인 비선형 변환 수행\n",
    "        ffn_out = self.ffn(out2)\n",
    "        ffn_out = self.dropout3(ffn_out)\n",
    "        out3 = self.norm3(out2 + ffn_out)  # 잔차 연결 + LayerNorm\n",
    "\n",
    "        return out3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "29d0f84e-9aef-4805-a1f2-60ea645d3f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 num_layers,\n",
    "                 ff_dim,\n",
    "                 d_model,\n",
    "                 num_heads,\n",
    "                 dropout=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # (1) 토큰 임베딩 레이어\n",
    "        # 입력된 토큰 인덱스(정수 시퀀스)를 d_model 차원의 임베딩 벡터로 변환\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=sp.pad_id())\n",
    "\n",
    "        # (2) 포지셔널 인코딩 (Positional Encoding)\n",
    "        # 디코더 입력 시퀀스의 위치 정보를 벡터에 더해줌\n",
    "        # ※ 실제 구현에서는 vocab_size 대신 max_seq_len을 사용하는 경우가 많음\n",
    "        self.pos_encoding = PositionalEncoding(position=vocab_size, d_model=d_model)\n",
    "\n",
    "        # (3) 드롭아웃\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # (4) DecoderLayer를 num_layers만큼 스택\n",
    "        # 각 레이어는 (Self-Attention → Encoder-Decoder Attention → FFN) 순서로 구성\n",
    "        self.dec_layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads, ff_dim, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, enc_outputs, look_ahead_mask=None, padding_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: 디코더 입력 (batch_size, target_seq_len)\n",
    "            enc_outputs: 인코더 출력 (batch_size, input_seq_len, d_model)\n",
    "            look_ahead_mask: 미래 단어 마스크 (디코더 셀프 어텐션용)\n",
    "            padding_mask: 인코더 출력의 패딩 위치 마스크 (Cross-Attention용)\n",
    "        Returns:\n",
    "            x: 디코더 출력 (batch_size, target_seq_len, d_model)\n",
    "        \"\"\"\n",
    "\n",
    "        # (1) 임베딩 + 스케일링\n",
    "        # 임베딩 결과를 √d_model로 스케일링해 학습 안정화\n",
    "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
    "\n",
    "        # (2) 포지셔널 인코딩 추가 + 드롭아웃\n",
    "        # 위치 정보(positional encoding)를 더해 순서 정보 보존\n",
    "        x = self.pos_encoding(x)    # (batch_size, tgt_seq_len, d_model)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # (3) 여러 개의 DecoderLayer를 순차적으로 통과\n",
    "        # 각 layer는 Masked Self-Attention → Cross-Attention → FFN 구조\n",
    "        for layer in self.dec_layers:\n",
    "            x = layer(x, enc_outputs, look_ahead_mask, padding_mask)\n",
    "\n",
    "        # (4) 디코더의 최종 출력 반환\n",
    "        # 각 타임스텝별로 문맥이 반영된 벡터 (d_model 차원)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6e25740f-f1a4-48e3-910d-8dbe4a4c740d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 num_layers,      # 인코더/디코더 층 수\n",
    "                 units,           # Feed-Forward Network의 은닉 차원 (ff_dim)\n",
    "                 d_model,         # 임베딩 및 내부 표현 차원\n",
    "                 num_heads,       # 멀티헤드 어텐션 헤드 수\n",
    "                 dropout=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        # 1) 인코더 초기화\n",
    "        self.encoder = Encoder(\n",
    "            vocab_size=vocab_size,\n",
    "            num_layers=num_layers,\n",
    "            ff_dim=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        # 2) 디코더 초기화\n",
    "        self.decoder = Decoder(\n",
    "            vocab_size=vocab_size,\n",
    "            num_layers=num_layers,\n",
    "            ff_dim=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        # 3) 최종 출력층: d_model -> vocab_size\n",
    "        # 디코더 출력 벡터를 단어 확률로 변환\n",
    "        self.final_linear = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, inputs, dec_inputs):\n",
    "        \"\"\"\n",
    "        inputs: 인코더 입력 (batch_size, src_seq_len)\n",
    "        dec_inputs: 디코더 입력 (batch_size, tgt_seq_len)\n",
    "        \"\"\"\n",
    "        # 1) 인코더 패딩 마스크 생성\n",
    "        # PAD 토큰 위치는 1, 실제 토큰은 0\n",
    "        enc_padding_mask = create_padding_mask(inputs)     # shape: (batch_size, 1, 1, src_seq_len)\n",
    "\n",
    "        # 2) 디코더 Look-Ahead Mask + 패딩 마스크\n",
    "        # 디코더에서 미래 토큰을 보지 않도록 상삼각 마스크 적용\n",
    "        look_ahead_mask = create_look_ahead_mask(dec_inputs)  # shape: (batch_size, 1, tgt_seq_len, tgt_seq_len)\n",
    "\n",
    "        # 3) 디코더에서 인코더 출력 쪽을 마스킹할 때 사용\n",
    "        # 인코더 입력의 PAD 토큰 위치를 마스크\n",
    "        dec_padding_mask = create_padding_mask(inputs)        # shape: (batch_size, 1, 1, src_seq_len)\n",
    "\n",
    "        # 4) 인코더 수행\n",
    "        enc_outputs = self.encoder(\n",
    "            x=inputs,\n",
    "            mask=enc_padding_mask\n",
    "        )  # shape: (batch_size, src_seq_len, d_model)\n",
    "\n",
    "        # 5) 디코더 수행\n",
    "        dec_outputs = self.decoder(\n",
    "            x=dec_inputs,           # 디코더 입력 (batch_size, tgt_seq_len)\n",
    "            enc_outputs=enc_outputs,# 인코더 출력 (batch_size, src_seq_len, d_model)\n",
    "            look_ahead_mask=look_ahead_mask,\n",
    "            padding_mask=dec_padding_mask\n",
    "        )  # shape: (batch_size, tgt_seq_len, d_model)\n",
    "\n",
    "        # 6) 최종 Linear 레이어: d_model -> vocab_size\n",
    "        # 각 위치마다 단어 확률(logits) 출력\n",
    "        logits = self.final_linear(dec_outputs)  # shape: (batch_size, tgt_seq_len, vocab_size)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80bbdfa-f794-4a63-8a56-0074126507f0",
   "metadata": {},
   "source": [
    "## Step 4-2. Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "102e9701-7a2f-4d49-bdf7-d47aea72cc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KoreanChatDataset(Dataset):\n",
    "    def __init__(self, pairs, sp, max_length=40):\n",
    "        \"\"\"\n",
    "        한국어 챗봇 병렬 데이터셋 클래스\n",
    "\n",
    "        Args:\n",
    "            pairs (list of tuple): (질문, 답변) 쌍의 리스트\n",
    "            sp (SentencePieceProcessor): 학습된 SentencePiece 토크나이저\n",
    "            max_length (int): 최대 문장 길이 (패딩 포함)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.sp = sp\n",
    "        self.max_length = max_length\n",
    "        self.data = []\n",
    "\n",
    "        for q_text, a_text in pairs:\n",
    "            # 전처리: 불필요한 공백 및 특수문자 제거\n",
    "            q_text = q_text.strip()\n",
    "            a_text = a_text.strip()\n",
    "            if not q_text or not a_text:\n",
    "                continue\n",
    "\n",
    "            # SentencePiece 토큰화 (문자열 → ID 시퀀스)\n",
    "            q_ids = sp.encode(q_text, out_type=int)\n",
    "            a_ids = sp.encode(a_text, out_type=int)\n",
    "\n",
    "            # 시작/종료 토큰 추가 (없을 경우 기본값 지정)\n",
    "            bos_id = sp.bos_id() if sp.bos_id() >= 0 else 1\n",
    "            eos_id = sp.eos_id() if sp.eos_id() >= 0 else 2\n",
    "            pad_id = sp.pad_id() if sp.pad_id() >= 0 else 0\n",
    "\n",
    "            q_tokens = [bos_id] + q_ids + [eos_id]\n",
    "            a_tokens = [bos_id] + a_ids + [eos_id]\n",
    "\n",
    "            # 최대 길이 초과 문장 제외\n",
    "            if len(q_tokens) > max_length or len(a_tokens) > max_length:\n",
    "                continue\n",
    "\n",
    "            # 패딩 추가 (max_length에 맞춰 0으로 채움)\n",
    "            q_tokens += [pad_id] * (max_length - len(q_tokens))\n",
    "            a_tokens += [pad_id] * (max_length - len(a_tokens))\n",
    "\n",
    "            # 디코더 입력(dec_input)과 타깃(target) 분리 (Teacher Forcing)\n",
    "            dec_input = a_tokens[:-1]  # <BOS>로 시작, 마지막 토큰 제외\n",
    "            target = a_tokens[1:]      # 첫 토큰 제외, <EOS>까지 포함\n",
    "\n",
    "            # 데이터 저장\n",
    "            self.data.append({\n",
    "                \"enc_input\": q_tokens,  # Encoder 입력\n",
    "                \"dec_input\": dec_input,  # Decoder 입력\n",
    "                \"target\": target         # Decoder 타깃\n",
    "            })\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"전체 샘플 수 반환\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"인덱스에 해당하는 샘플 반환\"\"\"\n",
    "        sample = self.data[idx]\n",
    "        enc_input = torch.tensor(sample[\"enc_input\"], dtype=torch.long)\n",
    "        dec_input = torch.tensor(sample[\"dec_input\"], dtype=torch.long)\n",
    "        target = torch.tensor(sample[\"target\"], dtype=torch.long)\n",
    "        return enc_input, dec_input, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "618ecd83-b17a-4332-a74b-59ee05ee3983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장길이 평균 :  12.893299406276505\n",
      "문장길이 최대 :  56\n",
      "문장길이 표준편차 :  6.162788620730713\n",
      "적정 문장 길이 : 25\n",
      "전체 문장의 0.96%가 maxlen 설정값 이내에 포함됩니다. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(12.893299406276505),\n",
       " np.int64(56),\n",
       " np.float64(6.162788620730713),\n",
       " 25)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sequence_len(data):\n",
    "    \"\"\"\n",
    "    문장길이의 평균값, 최대값, 표준편차를 계산해 본다.\n",
    "    \"\"\"\n",
    "    num_tokens = [len(tokens) for tokens in data]\n",
    "    num_tokens = np.array(num_tokens)\n",
    "\n",
    "    sequence_mean = np.mean(num_tokens)\n",
    "    sequence_max = np.max(num_tokens)\n",
    "    sequence_std = np.std(num_tokens)\n",
    "    \n",
    "    print('문장길이 평균 : ', sequence_mean)\n",
    "    print('문장길이 최대 : ', sequence_max)\n",
    "    print('문장길이 표준편차 : ', sequence_std)\n",
    "\n",
    "    max_tokens = sequence_mean + 2 * sequence_std\n",
    "    maxlen = int(max_tokens)\n",
    "    print(f'적정 문장 길이 : {maxlen}')\n",
    "    print(f'전체 문장의 {np.sum([len(tokens) for tokens in data] < max_tokens) / len(num_tokens):.2f}%가 maxlen 설정값 이내에 포함됩니다. ')\n",
    "    return sequence_mean, sequence_max, sequence_std, maxlen\n",
    "    \n",
    "sequence_len(data.Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4c1b8bd3-8d77-4d93-90ff-b5be3536a045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장길이 평균 :  15.050296861747244\n",
      "문장길이 최대 :  76\n",
      "문장길이 표준편차 :  6.6777209494680045\n",
      "적정 문장 길이 : 28\n",
      "전체 문장의 0.96%가 maxlen 설정값 이내에 포함됩니다. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(15.050296861747244),\n",
       " np.int64(76),\n",
       " np.float64(6.6777209494680045),\n",
       " 28)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_len(data.A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9c95e389-875c-472d-97ff-0f79ed57fe8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = KoreanChatDataset(pairs, sp, max_length=28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6e77b586-b70a-4293-8a8f-885851e9ad5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f382f8b9-f4a7-4530-a17f-ccb70c9fbc03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 28])\n",
      "torch.Size([32, 27])\n",
      "torch.Size([32, 27])\n"
     ]
    }
   ],
   "source": [
    "for encoder_input, decoder_input, decoder_label in dataloader:\n",
    "    print(encoder_input.size())\n",
    "    print(decoder_input.size())\n",
    "    print(decoder_label.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5b406e-607d-454b-ab5c-b8b5fec7abbe",
   "metadata": {},
   "source": [
    "## Step 4-3. Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "772226f2-7d58-48fe-9c58-9b51f904e7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시: 하이퍼파라미터 설정\n",
    "NUM_LAYERS = 6      # 인코더/디코더 층 수\n",
    "D_MODEL = 256       # 임베딩 및 내부 표현 차원\n",
    "NUM_HEADS = 8       # 멀티헤드 어텐션 헤드 수\n",
    "UNITS = 512         # Feed-Forward Network 은닉 차원 (ff_dim)\n",
    "DROPOUT = 0.1       # 드롭아웃 비율\n",
    "VOCAB_SIZE = 1200   # 단어 집합 크기 (예시)\n",
    "\n",
    "model = Transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dffb3aa4-ea5d-4d0a-ae00-da4efb7d8bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss(ignore_index=sp.pad_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "81163c61-67c9-4a71-a4c0-4766ac3fdb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_lambda(d_model, warmup_steps=4000):\n",
    "    \"\"\"\n",
    "    Transformer 학습 시 사용되는 학습률 스케줄 함수(Step-wise learning rate scheduler) 생성.\n",
    "\n",
    "    d_model: 모델 내부 표현 차원 (embedding 차원)\n",
    "    warmup_steps: 학습률 warm-up 단계 수\n",
    "\n",
    "    반환값: 현재 step에 따른 학습률 비율 계산 함수\n",
    "    \"\"\"\n",
    "    d_model = float(d_model)\n",
    "\n",
    "    def lr_lambda(step):\n",
    "        # step은 0부터 시작하므로 +1로 보정\n",
    "        step = step + 1\n",
    "\n",
    "        # 수식: d_model^-0.5 * min(step^-0.5, step * warmup_steps^-1.5)\n",
    "        # - 초기 단계(warmup)에서는 step * warmup_steps^-1.5가 작아 학습률 점점 증가\n",
    "        # - warmup 이후에는 step^-0.5가 작아져 학습률 점점 감소\n",
    "        return (d_model ** -0.5) * min(step ** -0.5, step * (warmup_steps ** -1.5))\n",
    "\n",
    "    return lr_lambda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ee93c2c8-ca14-49d2-9dc6-fbe916e3e4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer 정의\n",
    "optimizer = optim.Adam(model.parameters(), betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "# Scheduler 정의\n",
    "scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=get_lr_lambda(D_MODEL, warmup_steps=4000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ba3c2ca2-6797-4b4d-9031-8ec50abdba03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_function(y_pred, y_true, pad_id=0):\n",
    "    \"\"\"\n",
    "    모델 예측과 실제 정답을 비교하여 정확도를 계산합니다.\n",
    "    \n",
    "    Args:\n",
    "        y_pred (Tensor): 모델 출력, shape = (batch_size, seq_len, vocab_size)\n",
    "        y_true (Tensor): 실제 정답 토큰, shape = (batch_size, seq_len)\n",
    "        pad_id (int): 패딩 토큰 ID (정답에 포함되어도 정확도 계산에서 제외)\n",
    "    \n",
    "    Returns:\n",
    "        acc (Tensor): 배치 단위 정확도 (0~1)\n",
    "    \"\"\"\n",
    "    # 1) vocab 차원에서 가장 높은 확률을 가진 토큰 선택\n",
    "    preds = y_pred.argmax(dim=-1)  # shape = (batch_size, seq_len)\n",
    "\n",
    "    # 2) 패딩이 아닌 위치만 선택\n",
    "    mask = (y_true != pad_id)      # shape = (batch_size, seq_len)\n",
    "\n",
    "    # 3) 예측이 정답과 같고, 패딩이 아닌 위치만 True\n",
    "    correct = (preds == y_true) & mask\n",
    "\n",
    "    # 4) 정확도 = 맞춘 토큰 수 / 패딩 제외 전체 토큰 수\n",
    "    acc = correct.float().sum() / mask.float().sum()\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "53b8c0ce-c36f-4c79-84d5-fd6218512975",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "344bf7c1-7eed-4efe-a1e5-100fc6d9467c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, batch, optimizer, loss_function, device):\n",
    "    \"\"\"\n",
    "    한 배치(batch)에 대한 학습 단계 수행\n",
    "    \"\"\"\n",
    "    model.train()  \n",
    "    enc_input, dec_input, target = [x.to(device) for x in batch] \n",
    "\n",
    "    optimizer.zero_grad()  # 이전 gradient 초기화\n",
    "\n",
    "    # 1) 모델 forward pass: (batch_size, seq_len, vocab_size) 출력\n",
    "    logits = model(enc_input, dec_input)\n",
    "\n",
    "    # 2) Loss 계산\n",
    "    # CrossEntropyLoss는 (batch_size, vocab_size, seq_len) 형태 필요\n",
    "    # pad_id 위치는 loss 계산에서 무시 가능\n",
    "    loss = loss_function(logits.permute(0, 2, 1), target)\n",
    "\n",
    "    # 3) Backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 4) 정확도 계산 (패딩 토큰 제외)\n",
    "    acc = accuracy_function(logits, target, pad_id=sp.pad_id())\n",
    "\n",
    "    return loss.item(), acc\n",
    "\n",
    "def train(model, dataloader, optimizer, loss_function, scheduler, num_epochs, device):\n",
    "    \"\"\"\n",
    "    전체 학습 루프\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss, total_acc = 0, 0\n",
    "\n",
    "        for step, batch in enumerate(dataloader):\n",
    "            # 배치 학습 수행\n",
    "            loss, acc = train_step(model, batch, optimizer, loss_function, device)\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "\n",
    "            # 100 스텝마다 로그 출력\n",
    "            if step % 100 == 0:\n",
    "                print(f\"[Epoch {epoch+1}, Step {step}] Loss: {loss:.4f}, Acc: {acc:.4f}\")\n",
    "\n",
    "            # 학습률 스케줄러 업데이트 (step 단위)\n",
    "            scheduler.step()\n",
    "\n",
    "        # 에포크 평균 Loss, Accuracy 출력\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        avg_acc = total_acc / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1} Completed - Avg Loss: {avg_loss:.4f}, Avg Acc: {avg_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6bb463ec-6d91-4d61-a51d-f9e84a49436b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, Step 0] Loss: 7.3226, Acc: 0.0000\n",
      "[Epoch 1, Step 100] Loss: 7.3438, Acc: 0.0000\n",
      "[Epoch 1, Step 200] Loss: 7.2717, Acc: 0.0000\n",
      "[Epoch 1, Step 300] Loss: 7.2984, Acc: 0.0000\n",
      "Epoch 1 Completed - Avg Loss: 7.3194, Avg Acc: 0.0006\n",
      "[Epoch 2, Step 0] Loss: 7.2797, Acc: 0.0000\n",
      "[Epoch 2, Step 100] Loss: 7.2320, Acc: 0.0025\n",
      "[Epoch 2, Step 200] Loss: 7.1560, Acc: 0.0000\n",
      "[Epoch 2, Step 300] Loss: 7.1279, Acc: 0.0000\n",
      "Epoch 2 Completed - Avg Loss: 7.1743, Avg Acc: 0.0007\n",
      "[Epoch 3, Step 0] Loss: 7.0592, Acc: 0.0000\n",
      "[Epoch 3, Step 100] Loss: 6.9617, Acc: 0.0045\n",
      "[Epoch 3, Step 200] Loss: 6.8840, Acc: 0.0054\n",
      "[Epoch 3, Step 300] Loss: 6.8219, Acc: 0.0501\n",
      "Epoch 3 Completed - Avg Loss: 6.8947, Avg Acc: 0.0261\n",
      "[Epoch 4, Step 0] Loss: 6.7224, Acc: 0.0956\n",
      "[Epoch 4, Step 100] Loss: 6.6300, Acc: 0.1469\n",
      "[Epoch 4, Step 200] Loss: 6.4844, Acc: 0.1537\n",
      "[Epoch 4, Step 300] Loss: 6.4286, Acc: 0.1637\n",
      "Epoch 4 Completed - Avg Loss: 6.5178, Avg Acc: 0.1471\n",
      "[Epoch 5, Step 0] Loss: 6.3549, Acc: 0.1622\n",
      "[Epoch 5, Step 100] Loss: 6.2391, Acc: 0.1533\n",
      "[Epoch 5, Step 200] Loss: 6.0431, Acc: 0.1782\n",
      "[Epoch 5, Step 300] Loss: 5.9977, Acc: 0.1807\n",
      "Epoch 5 Completed - Avg Loss: 6.1399, Avg Acc: 0.1651\n",
      "[Epoch 6, Step 0] Loss: 5.9200, Acc: 0.1886\n",
      "[Epoch 6, Step 100] Loss: 5.8777, Acc: 0.1680\n",
      "[Epoch 6, Step 200] Loss: 5.7446, Acc: 0.1694\n",
      "[Epoch 6, Step 300] Loss: 5.7280, Acc: 0.1811\n",
      "Epoch 6 Completed - Avg Loss: 5.8618, Avg Acc: 0.1727\n",
      "[Epoch 7, Step 0] Loss: 5.7107, Acc: 0.2000\n",
      "[Epoch 7, Step 100] Loss: 5.8417, Acc: 0.1893\n",
      "[Epoch 7, Step 200] Loss: 5.6794, Acc: 0.2249\n",
      "[Epoch 7, Step 300] Loss: 5.7144, Acc: 0.2109\n",
      "Epoch 7 Completed - Avg Loss: 5.6895, Avg Acc: 0.2131\n",
      "[Epoch 8, Step 0] Loss: 5.7749, Acc: 0.2099\n",
      "[Epoch 8, Step 100] Loss: 5.5166, Acc: 0.2423\n",
      "[Epoch 8, Step 200] Loss: 5.6015, Acc: 0.2323\n",
      "[Epoch 8, Step 300] Loss: 5.5398, Acc: 0.2228\n",
      "Epoch 8 Completed - Avg Loss: 5.5718, Avg Acc: 0.2301\n",
      "[Epoch 9, Step 0] Loss: 5.5341, Acc: 0.2271\n",
      "[Epoch 9, Step 100] Loss: 5.4875, Acc: 0.2407\n",
      "[Epoch 9, Step 200] Loss: 5.4354, Acc: 0.2425\n",
      "[Epoch 9, Step 300] Loss: 5.4999, Acc: 0.2484\n",
      "Epoch 9 Completed - Avg Loss: 5.4713, Avg Acc: 0.2409\n",
      "[Epoch 10, Step 0] Loss: 5.3741, Acc: 0.2615\n",
      "[Epoch 10, Step 100] Loss: 5.4305, Acc: 0.2571\n",
      "[Epoch 10, Step 200] Loss: 5.2695, Acc: 0.2807\n",
      "[Epoch 10, Step 300] Loss: 5.4372, Acc: 0.2522\n",
      "Epoch 10 Completed - Avg Loss: 5.3736, Avg Acc: 0.2537\n",
      "[Epoch 11, Step 0] Loss: 5.2795, Acc: 0.2765\n",
      "[Epoch 11, Step 100] Loss: 5.3902, Acc: 0.2506\n",
      "[Epoch 11, Step 200] Loss: 5.2212, Acc: 0.2650\n",
      "[Epoch 11, Step 300] Loss: 5.2442, Acc: 0.2565\n",
      "Epoch 11 Completed - Avg Loss: 5.2788, Avg Acc: 0.2624\n",
      "[Epoch 12, Step 0] Loss: 5.3227, Acc: 0.2505\n",
      "[Epoch 12, Step 100] Loss: 5.1994, Acc: 0.2689\n",
      "[Epoch 12, Step 200] Loss: 5.2216, Acc: 0.2665\n",
      "[Epoch 12, Step 300] Loss: 5.0844, Acc: 0.2869\n",
      "Epoch 12 Completed - Avg Loss: 5.1900, Avg Acc: 0.2684\n",
      "[Epoch 13, Step 0] Loss: 5.2081, Acc: 0.2633\n",
      "[Epoch 13, Step 100] Loss: 5.0290, Acc: 0.2897\n",
      "[Epoch 13, Step 200] Loss: 5.2612, Acc: 0.2692\n",
      "[Epoch 13, Step 300] Loss: 5.0527, Acc: 0.2878\n",
      "Epoch 13 Completed - Avg Loss: 5.1151, Avg Acc: 0.2720\n",
      "[Epoch 14, Step 0] Loss: 5.1187, Acc: 0.2716\n",
      "[Epoch 14, Step 100] Loss: 5.1590, Acc: 0.2679\n",
      "[Epoch 14, Step 200] Loss: 4.9121, Acc: 0.2926\n",
      "[Epoch 14, Step 300] Loss: 5.0834, Acc: 0.2542\n",
      "Epoch 14 Completed - Avg Loss: 5.0497, Avg Acc: 0.2756\n",
      "[Epoch 15, Step 0] Loss: 5.0593, Acc: 0.2652\n",
      "[Epoch 15, Step 100] Loss: 4.9833, Acc: 0.2882\n",
      "[Epoch 15, Step 200] Loss: 4.8613, Acc: 0.3085\n",
      "[Epoch 15, Step 300] Loss: 4.9548, Acc: 0.2903\n",
      "Epoch 15 Completed - Avg Loss: 4.9939, Avg Acc: 0.2787\n",
      "[Epoch 16, Step 0] Loss: 4.9117, Acc: 0.2938\n",
      "[Epoch 16, Step 100] Loss: 4.9029, Acc: 0.2857\n",
      "[Epoch 16, Step 200] Loss: 4.9301, Acc: 0.2812\n",
      "[Epoch 16, Step 300] Loss: 4.8775, Acc: 0.2816\n",
      "Epoch 16 Completed - Avg Loss: 4.9429, Avg Acc: 0.2813\n",
      "[Epoch 17, Step 0] Loss: 4.9143, Acc: 0.2835\n",
      "[Epoch 17, Step 100] Loss: 4.9333, Acc: 0.2697\n",
      "[Epoch 17, Step 200] Loss: 4.8562, Acc: 0.2865\n",
      "[Epoch 17, Step 300] Loss: 4.8146, Acc: 0.2835\n",
      "Epoch 17 Completed - Avg Loss: 4.8980, Avg Acc: 0.2835\n",
      "[Epoch 18, Step 0] Loss: 4.8891, Acc: 0.2836\n",
      "[Epoch 18, Step 100] Loss: 4.9906, Acc: 0.2775\n",
      "[Epoch 18, Step 200] Loss: 4.8159, Acc: 0.2861\n",
      "[Epoch 18, Step 300] Loss: 4.8517, Acc: 0.2874\n",
      "Epoch 18 Completed - Avg Loss: 4.8560, Avg Acc: 0.2856\n",
      "[Epoch 19, Step 0] Loss: 4.8894, Acc: 0.2766\n",
      "[Epoch 19, Step 100] Loss: 4.7755, Acc: 0.2861\n",
      "[Epoch 19, Step 200] Loss: 4.7519, Acc: 0.2986\n",
      "[Epoch 19, Step 300] Loss: 4.7729, Acc: 0.2983\n",
      "Epoch 19 Completed - Avg Loss: 4.8169, Avg Acc: 0.2882\n",
      "[Epoch 20, Step 0] Loss: 4.7403, Acc: 0.2997\n",
      "[Epoch 20, Step 100] Loss: 4.7002, Acc: 0.3132\n",
      "[Epoch 20, Step 200] Loss: 4.6896, Acc: 0.3011\n",
      "[Epoch 20, Step 300] Loss: 4.7632, Acc: 0.2940\n",
      "Epoch 20 Completed - Avg Loss: 4.7798, Avg Acc: 0.2898\n",
      "[Epoch 21, Step 0] Loss: 4.6691, Acc: 0.3163\n",
      "[Epoch 21, Step 100] Loss: 4.7764, Acc: 0.2720\n",
      "[Epoch 21, Step 200] Loss: 4.7353, Acc: 0.3019\n",
      "[Epoch 21, Step 300] Loss: 4.7111, Acc: 0.3047\n",
      "Epoch 21 Completed - Avg Loss: 4.7453, Avg Acc: 0.2917\n",
      "[Epoch 22, Step 0] Loss: 4.6460, Acc: 0.3099\n",
      "[Epoch 22, Step 100] Loss: 4.7852, Acc: 0.2905\n",
      "[Epoch 22, Step 200] Loss: 4.7115, Acc: 0.2857\n",
      "[Epoch 22, Step 300] Loss: 4.7035, Acc: 0.3039\n",
      "Epoch 22 Completed - Avg Loss: 4.7132, Avg Acc: 0.2930\n",
      "[Epoch 23, Step 0] Loss: 4.7129, Acc: 0.2917\n",
      "[Epoch 23, Step 100] Loss: 4.6682, Acc: 0.2957\n",
      "[Epoch 23, Step 200] Loss: 4.6605, Acc: 0.3075\n",
      "[Epoch 23, Step 300] Loss: 4.5907, Acc: 0.3015\n",
      "Epoch 23 Completed - Avg Loss: 4.6841, Avg Acc: 0.2946\n",
      "[Epoch 24, Step 0] Loss: 4.5712, Acc: 0.3002\n",
      "[Epoch 24, Step 100] Loss: 4.6211, Acc: 0.3146\n",
      "[Epoch 24, Step 200] Loss: 4.5436, Acc: 0.3248\n",
      "[Epoch 24, Step 300] Loss: 4.6181, Acc: 0.3112\n",
      "Epoch 24 Completed - Avg Loss: 4.6562, Avg Acc: 0.2958\n",
      "[Epoch 25, Step 0] Loss: 4.6578, Acc: 0.2909\n",
      "[Epoch 25, Step 100] Loss: 4.6673, Acc: 0.2801\n",
      "[Epoch 25, Step 200] Loss: 4.6147, Acc: 0.2967\n",
      "[Epoch 25, Step 300] Loss: 4.5603, Acc: 0.2903\n",
      "Epoch 25 Completed - Avg Loss: 4.6290, Avg Acc: 0.2976\n",
      "[Epoch 26, Step 0] Loss: 4.7709, Acc: 0.2897\n",
      "[Epoch 26, Step 100] Loss: 4.6743, Acc: 0.2972\n",
      "[Epoch 26, Step 200] Loss: 4.6458, Acc: 0.2899\n",
      "[Epoch 26, Step 300] Loss: 4.6327, Acc: 0.2815\n",
      "Epoch 26 Completed - Avg Loss: 4.6047, Avg Acc: 0.2985\n",
      "[Epoch 27, Step 0] Loss: 4.5717, Acc: 0.3023\n",
      "[Epoch 27, Step 100] Loss: 4.4804, Acc: 0.3183\n",
      "[Epoch 27, Step 200] Loss: 4.4563, Acc: 0.3073\n",
      "[Epoch 27, Step 300] Loss: 4.4101, Acc: 0.3283\n",
      "Epoch 27 Completed - Avg Loss: 4.5811, Avg Acc: 0.3000\n",
      "[Epoch 28, Step 0] Loss: 4.4271, Acc: 0.3122\n",
      "[Epoch 28, Step 100] Loss: 4.5351, Acc: 0.3136\n",
      "[Epoch 28, Step 200] Loss: 4.4932, Acc: 0.3073\n",
      "[Epoch 28, Step 300] Loss: 4.5041, Acc: 0.2932\n",
      "Epoch 28 Completed - Avg Loss: 4.5583, Avg Acc: 0.3011\n",
      "[Epoch 29, Step 0] Loss: 4.3725, Acc: 0.3095\n",
      "[Epoch 29, Step 100] Loss: 4.4985, Acc: 0.3099\n",
      "[Epoch 29, Step 200] Loss: 4.4218, Acc: 0.3206\n",
      "[Epoch 29, Step 300] Loss: 4.6946, Acc: 0.2762\n",
      "Epoch 29 Completed - Avg Loss: 4.5375, Avg Acc: 0.3026\n",
      "[Epoch 30, Step 0] Loss: 4.4509, Acc: 0.3005\n",
      "[Epoch 30, Step 100] Loss: 4.3942, Acc: 0.3103\n",
      "[Epoch 30, Step 200] Loss: 4.4859, Acc: 0.2982\n",
      "[Epoch 30, Step 300] Loss: 4.5072, Acc: 0.2941\n",
      "Epoch 30 Completed - Avg Loss: 4.5183, Avg Acc: 0.3031\n",
      "[Epoch 31, Step 0] Loss: 4.5042, Acc: 0.2974\n",
      "[Epoch 31, Step 100] Loss: 4.5328, Acc: 0.2921\n",
      "[Epoch 31, Step 200] Loss: 4.2928, Acc: 0.3359\n",
      "[Epoch 31, Step 300] Loss: 4.4156, Acc: 0.3099\n",
      "Epoch 31 Completed - Avg Loss: 4.4996, Avg Acc: 0.3043\n",
      "[Epoch 32, Step 0] Loss: 4.4636, Acc: 0.3135\n",
      "[Epoch 32, Step 100] Loss: 4.3917, Acc: 0.3171\n",
      "[Epoch 32, Step 200] Loss: 4.4897, Acc: 0.3017\n",
      "[Epoch 32, Step 300] Loss: 4.5481, Acc: 0.2809\n",
      "Epoch 32 Completed - Avg Loss: 4.4827, Avg Acc: 0.3051\n",
      "[Epoch 33, Step 0] Loss: 4.5258, Acc: 0.2967\n",
      "[Epoch 33, Step 100] Loss: 4.4665, Acc: 0.3103\n",
      "[Epoch 33, Step 200] Loss: 4.3672, Acc: 0.3032\n",
      "[Epoch 33, Step 300] Loss: 4.3308, Acc: 0.3307\n",
      "Epoch 33 Completed - Avg Loss: 4.4658, Avg Acc: 0.3058\n",
      "[Epoch 34, Step 0] Loss: 4.3635, Acc: 0.3143\n",
      "[Epoch 34, Step 100] Loss: 4.3918, Acc: 0.3067\n",
      "[Epoch 34, Step 200] Loss: 4.4497, Acc: 0.3029\n",
      "[Epoch 34, Step 300] Loss: 4.3273, Acc: 0.3273\n",
      "Epoch 34 Completed - Avg Loss: 4.4488, Avg Acc: 0.3066\n",
      "[Epoch 35, Step 0] Loss: 4.3097, Acc: 0.3275\n",
      "[Epoch 35, Step 100] Loss: 4.3346, Acc: 0.3169\n",
      "[Epoch 35, Step 200] Loss: 4.4813, Acc: 0.3041\n",
      "[Epoch 35, Step 300] Loss: 4.4081, Acc: 0.3064\n",
      "Epoch 35 Completed - Avg Loss: 4.4345, Avg Acc: 0.3075\n",
      "[Epoch 36, Step 0] Loss: 4.4447, Acc: 0.2997\n",
      "[Epoch 36, Step 100] Loss: 4.3623, Acc: 0.3105\n",
      "[Epoch 36, Step 200] Loss: 4.4104, Acc: 0.3084\n",
      "[Epoch 36, Step 300] Loss: 4.3180, Acc: 0.3135\n",
      "Epoch 36 Completed - Avg Loss: 4.4200, Avg Acc: 0.3081\n",
      "[Epoch 37, Step 0] Loss: 4.4926, Acc: 0.2972\n",
      "[Epoch 37, Step 100] Loss: 4.4391, Acc: 0.3026\n",
      "[Epoch 37, Step 200] Loss: 4.5610, Acc: 0.2921\n",
      "[Epoch 37, Step 300] Loss: 4.3675, Acc: 0.2966\n",
      "Epoch 37 Completed - Avg Loss: 4.4053, Avg Acc: 0.3089\n",
      "[Epoch 38, Step 0] Loss: 4.3432, Acc: 0.3138\n",
      "[Epoch 38, Step 100] Loss: 4.4640, Acc: 0.2938\n",
      "[Epoch 38, Step 200] Loss: 4.4452, Acc: 0.3145\n",
      "[Epoch 38, Step 300] Loss: 4.4207, Acc: 0.3170\n",
      "Epoch 38 Completed - Avg Loss: 4.3934, Avg Acc: 0.3091\n",
      "[Epoch 39, Step 0] Loss: 4.2305, Acc: 0.3342\n",
      "[Epoch 39, Step 100] Loss: 4.3465, Acc: 0.3197\n",
      "[Epoch 39, Step 200] Loss: 4.3634, Acc: 0.3146\n",
      "[Epoch 39, Step 300] Loss: 4.3787, Acc: 0.3106\n",
      "Epoch 39 Completed - Avg Loss: 4.3796, Avg Acc: 0.3098\n",
      "[Epoch 40, Step 0] Loss: 4.3921, Acc: 0.3104\n",
      "[Epoch 40, Step 100] Loss: 4.3779, Acc: 0.3214\n",
      "[Epoch 40, Step 200] Loss: 4.2607, Acc: 0.3183\n",
      "[Epoch 40, Step 300] Loss: 4.3484, Acc: 0.3162\n",
      "Epoch 40 Completed - Avg Loss: 4.3685, Avg Acc: 0.3104\n",
      "[Epoch 41, Step 0] Loss: 4.3321, Acc: 0.3108\n",
      "[Epoch 41, Step 100] Loss: 4.4280, Acc: 0.3012\n",
      "[Epoch 41, Step 200] Loss: 4.3046, Acc: 0.3170\n",
      "[Epoch 41, Step 300] Loss: 4.3399, Acc: 0.3063\n",
      "Epoch 41 Completed - Avg Loss: 4.3568, Avg Acc: 0.3109\n",
      "[Epoch 42, Step 0] Loss: 4.2716, Acc: 0.3261\n",
      "[Epoch 42, Step 100] Loss: 4.3120, Acc: 0.3141\n",
      "[Epoch 42, Step 200] Loss: 4.3332, Acc: 0.3188\n",
      "[Epoch 42, Step 300] Loss: 4.4277, Acc: 0.3017\n",
      "Epoch 42 Completed - Avg Loss: 4.3454, Avg Acc: 0.3111\n",
      "[Epoch 43, Step 0] Loss: 4.3793, Acc: 0.2826\n",
      "[Epoch 43, Step 100] Loss: 4.2827, Acc: 0.3039\n",
      "[Epoch 43, Step 200] Loss: 4.3903, Acc: 0.3115\n",
      "[Epoch 43, Step 300] Loss: 4.3187, Acc: 0.3020\n",
      "Epoch 43 Completed - Avg Loss: 4.3346, Avg Acc: 0.3118\n",
      "[Epoch 44, Step 0] Loss: 4.3389, Acc: 0.3266\n",
      "[Epoch 44, Step 100] Loss: 4.3177, Acc: 0.3031\n",
      "[Epoch 44, Step 200] Loss: 4.3060, Acc: 0.3023\n",
      "[Epoch 44, Step 300] Loss: 4.3567, Acc: 0.3028\n",
      "Epoch 44 Completed - Avg Loss: 4.3242, Avg Acc: 0.3122\n",
      "[Epoch 45, Step 0] Loss: 4.1967, Acc: 0.3475\n",
      "[Epoch 45, Step 100] Loss: 4.2309, Acc: 0.3290\n",
      "[Epoch 45, Step 200] Loss: 4.2699, Acc: 0.3194\n",
      "[Epoch 45, Step 300] Loss: 4.1364, Acc: 0.3619\n",
      "Epoch 45 Completed - Avg Loss: 4.3138, Avg Acc: 0.3123\n",
      "[Epoch 46, Step 0] Loss: 4.1796, Acc: 0.3197\n",
      "[Epoch 46, Step 100] Loss: 4.1863, Acc: 0.3308\n",
      "[Epoch 46, Step 200] Loss: 4.5520, Acc: 0.2898\n",
      "[Epoch 46, Step 300] Loss: 4.2411, Acc: 0.3289\n",
      "Epoch 46 Completed - Avg Loss: 4.3031, Avg Acc: 0.3132\n",
      "[Epoch 47, Step 0] Loss: 4.2223, Acc: 0.3138\n",
      "[Epoch 47, Step 100] Loss: 4.4459, Acc: 0.2925\n",
      "[Epoch 47, Step 200] Loss: 4.3641, Acc: 0.3085\n",
      "[Epoch 47, Step 300] Loss: 4.3173, Acc: 0.3051\n",
      "Epoch 47 Completed - Avg Loss: 4.2944, Avg Acc: 0.3133\n",
      "[Epoch 48, Step 0] Loss: 4.2414, Acc: 0.3352\n",
      "[Epoch 48, Step 100] Loss: 4.2679, Acc: 0.3173\n",
      "[Epoch 48, Step 200] Loss: 4.1313, Acc: 0.3243\n",
      "[Epoch 48, Step 300] Loss: 4.2008, Acc: 0.3369\n",
      "Epoch 48 Completed - Avg Loss: 4.2851, Avg Acc: 0.3135\n",
      "[Epoch 49, Step 0] Loss: 4.2160, Acc: 0.3275\n",
      "[Epoch 49, Step 100] Loss: 4.2341, Acc: 0.3171\n",
      "[Epoch 49, Step 200] Loss: 4.1235, Acc: 0.3260\n",
      "[Epoch 49, Step 300] Loss: 4.2996, Acc: 0.3068\n",
      "Epoch 49 Completed - Avg Loss: 4.2758, Avg Acc: 0.3139\n",
      "[Epoch 50, Step 0] Loss: 4.3062, Acc: 0.3205\n",
      "[Epoch 50, Step 100] Loss: 4.3581, Acc: 0.3053\n",
      "[Epoch 50, Step 200] Loss: 4.2454, Acc: 0.3169\n",
      "[Epoch 50, Step 300] Loss: 4.2927, Acc: 0.3086\n",
      "Epoch 50 Completed - Avg Loss: 4.2675, Avg Acc: 0.3141\n",
      "CPU times: user 20min 54s, sys: 3.35 s, total: 20min 57s\n",
      "Wall time: 16min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train(\n",
    "    model=model,\n",
    "    dataloader=dataloader,\n",
    "    optimizer=optimizer,\n",
    "    loss_function=loss_function,\n",
    "    scheduler=scheduler,\n",
    "    num_epochs=50,  # 원하는 에폭 수\n",
    "    device=device\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95fc366-af93-4ca6-bd6a-c40a1c1443a1",
   "metadata": {},
   "source": [
    "# Step 5. 모델 평가하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcc3538-6a62-4977-b94b-47ec6fdcece4",
   "metadata": {},
   "source": [
    "## Step 5-1. Greedy Search - 단순 가장 확률이 높은 단어 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d2fdf6dc-4778-4577-bd7e-0a9c974fb0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_inference(model, sentence, sp, device='cpu', max_length=33):\n",
    "    \"\"\"\n",
    "    한국어 SentencePiece 기반 Transformer 디코더 추론 (inference)\n",
    "\n",
    "    Args:\n",
    "        model: 학습된 Transformer 모델\n",
    "        sentence (str): 입력 문장 (자연어 텍스트)\n",
    "        sp: SentencePiece tokenizer\n",
    "        device (str): 'cpu' 또는 'cuda'\n",
    "        max_length (int): 생성할 최대 토큰 길이\n",
    "\n",
    "    Returns:\n",
    "        decoded_text (str): 모델이 생성한 최종 문장\n",
    "        output_ids (list[int]): 생성된 토큰 ID 시퀀스\n",
    "    \"\"\"\n",
    "\n",
    "    # 특수 토큰 ID 불러오기 (없을 경우 기본값 지정)\n",
    "    START_TOKEN = sp.bos_id() if sp.bos_id() >= 0 else 1\n",
    "    END_TOKEN   = sp.eos_id() if sp.eos_id() >= 0 else 2\n",
    "    PAD_TOKEN   = sp.pad_id() if sp.pad_id() >= 0 else 0\n",
    "\n",
    "    # 1. 입력 문장 전처리\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    # 2. 인코더 입력 구성\n",
    "    enc_input_ids = [START_TOKEN] + sp.encode(sentence, out_type=int) + [END_TOKEN]\n",
    "    enc_input = torch.tensor([enc_input_ids], dtype=torch.long, device=device)\n",
    "\n",
    "    # 3. 디코더 입력 초기화 (BOS만 포함)\n",
    "    dec_input = torch.tensor([[START_TOKEN]], dtype=torch.long, device=device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            # 4. Forward pass\n",
    "            logits = model(enc_input, dec_input)  # (1, seq_len, vocab_size)\n",
    "\n",
    "            # 5. 마지막 시점의 예측 결과만 사용\n",
    "            next_token_logits = logits[:, -1, :]  # (1, vocab_size)\n",
    "            predicted_id = torch.argmax(next_token_logits, dim=-1).item()\n",
    "\n",
    "            # 6. 종료 토큰이면 멈춤\n",
    "            if predicted_id == END_TOKEN:\n",
    "                break\n",
    "\n",
    "            # 7. 예측된 토큰을 디코더 입력에 추가\n",
    "            next_token = torch.tensor([[predicted_id]], device=device)\n",
    "            dec_input = torch.cat([dec_input, next_token], dim=1)\n",
    "\n",
    "    # 8. 최종 출력 시퀀스 정리\n",
    "    output_ids = dec_input.squeeze(0).tolist()\n",
    "\n",
    "    # 9. SentencePiece 디코딩 (BOS/EOS 제외)\n",
    "    if END_TOKEN in output_ids:\n",
    "        end_idx = output_ids.index(END_TOKEN)\n",
    "        output_ids = output_ids[1:end_idx]  # [BOS] ... [EOS]\n",
    "    else:\n",
    "        output_ids = output_ids[1:]  # EOS 없으면 끝까지\n",
    "\n",
    "    decoded_text = sp.decode(output_ids)\n",
    "\n",
    "    return decoded_text, output_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b829e0bb-caa8-4bfa-843d-929fac6da1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(model, sentence, sp, device='cpu'):\n",
    "    \"\"\"\n",
    "    학습된 Transformer 모델로 한국어 입력 문장에 대한 응답 문장 생성\n",
    "\n",
    "    Args:\n",
    "        model: 학습된 Transformer 모델\n",
    "        sentence (str): 입력 문장\n",
    "        sp: SentencePiece tokenizer\n",
    "        device (str): 'cpu' 또는 'cuda'\n",
    "\n",
    "    Returns:\n",
    "        predicted_sentence (str): 모델이 생성한 문장\n",
    "    \"\"\"\n",
    "    # 디코더 인퍼런스를 통해 예측된 문장 및 토큰 ID 시퀀스 획득\n",
    "    predicted_sentence, output_ids = decoder_inference(\n",
    "        model=model,\n",
    "        sentence=sentence,\n",
    "        sp=sp,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # 결과 출력\n",
    "    print(f\"입력: {sentence}\")\n",
    "    print(f\"출력: {predicted_sentence}\")\n",
    "    print(f\"토큰 ID 시퀀스: {output_ids}\")\n",
    "\n",
    "    return predicted_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ec27d490-7adb-412e-949b-78e82735129c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력: 훔쳐보는 것도 눈치 보임.\n",
      "출력: 가 가 가 가 가 가 가 가 가 가 가 가 가 가 가 가 \n",
      "토큰 ID 시퀀스: [4, 7, 4, 7, 4, 7, 4, 7, 4, 7, 4, 7, 4, 7, 4, 7, 4, 7, 4, 7, 4, 7, 4, 7, 4, 7, 4, 7, 4, 7, 4, 7, 4]\n"
     ]
    }
   ],
   "source": [
    "text = \"훔쳐보는 것도 눈치 보임.\"\n",
    "response = sentence_generation(model, text, sp, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7719b87a-8435-4997-b87b-0ea15c181fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력: 짝사랑만큼 고통스러운 건 없겠지.\n",
      "출력: 가 가 가 가 가 가 가 가 가 가 가 가 가 가 가 가 \n",
      "토큰 ID 시퀀스: [4, 7, 4, 7, 4, 7, 4, 7, 4, 7, 4, 7, 4, 7, 4, 7, 4, 7, 4, 7, 4, 7, 4, 7, 4, 7, 4, 7, 4, 7, 4, 7, 4]\n"
     ]
    }
   ],
   "source": [
    "text = \"짝사랑만큼 고통스러운 건 없겠지.\"\n",
    "response = sentence_generation(model, text, sp, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d559a359-9912-414d-a6b6-5a3aa5835df7",
   "metadata": {},
   "source": [
    "## Step 5-2. Beam Search - 후보 단어들을 유지하며 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fcab740b-b34c-4bd7-9d45-48fa48f0e72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_inference_beam(model, sentence, sp, device='cpu', max_length=33, beam_width=5):\n",
    "    START_TOKEN = sp.bos_id()\n",
    "    END_TOKEN = sp.eos_id()\n",
    "\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    enc_input_ids = [START_TOKEN] + sp.encode(sentence) + [END_TOKEN]\n",
    "    enc_input = torch.tensor([enc_input_ids], dtype=torch.long, device=device)\n",
    "\n",
    "    sequences = [[START_TOKEN]]\n",
    "    scores = [0.0]\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            all_candidates = []\n",
    "\n",
    "            for seq, score in zip(sequences, scores):\n",
    "                # 이미 종료된 시퀀스는 그대로 유지\n",
    "                if seq[-1] == END_TOKEN:\n",
    "                    all_candidates.append((score, seq))\n",
    "                    continue\n",
    "\n",
    "                dec_input = torch.tensor([seq], dtype=torch.long, device=device)\n",
    "                logits = model(enc_input, dec_input)\n",
    "                next_logits = logits[:, -1, :]\n",
    "                next_log_probs = torch.log_softmax(next_logits, dim=-1)\n",
    "\n",
    "                topk_log_probs, topk_ids = torch.topk(next_log_probs, beam_width, dim=-1)\n",
    "\n",
    "                for k in range(beam_width):\n",
    "                    next_token = int(topk_ids[0, k].item())\n",
    "                    candidate_seq = seq + [next_token]\n",
    "                    candidate_score = score + float(topk_log_probs[0, k].item())\n",
    "                    all_candidates.append((candidate_score, candidate_seq))\n",
    "\n",
    "            # 점수 기준 상위 후보 유지\n",
    "            all_candidates.sort(key=lambda x: x[0], reverse=True)\n",
    "            all_candidates = all_candidates[:beam_width]\n",
    "\n",
    "            sequences = [candidate[1] for candidate in all_candidates]\n",
    "            scores = [candidate[0] for candidate in all_candidates]\n",
    "\n",
    "            # 모든 후보 종료 시 반복 종료\n",
    "            if all(seq[-1] == END_TOKEN for seq in sequences):\n",
    "                break\n",
    "\n",
    "    best_sequence = sequences[0]\n",
    "    if best_sequence[-1] == END_TOKEN:\n",
    "        best_sequence = best_sequence[:-1]\n",
    "\n",
    "    return best_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f30f66e7-2528-4b85-bf76-cf7f084eefc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation_beam(model, sentence, sp, device='cpu'):\n",
    "    output_ids = decoder_inference_beam(model, sentence, sp, device=device)\n",
    "    predicted_sentence = sp.decode(output_ids)\n",
    "\n",
    "    print(f\"입력: {sentence}\")\n",
    "    print(f\"출력: {predicted_sentence}\")\n",
    "    print(f\"토큰 ID 시퀀스: {output_ids}\")\n",
    "\n",
    "    return predicted_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "aa8a9672-e54f-4f9e-b4d8-3e0733096800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력: 짝사랑만큼 고통스러운 건 없겠지.\n",
      "출력: 가 가 가 가 가요 .\n",
      "토큰 ID 시퀀스: [1, 4, 7, 4, 7, 4, 7, 4, 7, 4, 7, 8, 5]\n"
     ]
    }
   ],
   "source": [
    "text = \"짝사랑만큼 고통스러운 건 없겠지.\"\n",
    "response = sentence_generation_beam(model, text, sp, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e5c2a4e6-35f0-49f2-ba48-8c387513e34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력: 훔쳐보는 것도 눈치 보임.\n",
      "출력: 가 가 가 가 가 가 가요 .\n",
      "토큰 ID 시퀀스: [1, 4, 7, 4, 7, 4, 7, 4, 7, 4, 7, 4, 7, 4, 7, 8, 5]\n"
     ]
    }
   ],
   "source": [
    "text = \"훔쳐보는 것도 눈치 보임.\"\n",
    "response = sentence_generation_beam(model, text, sp, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "399d03b5-6f3c-4069-811b-9c638c280059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력: 12시 땡\n",
      "출력: 가 가 가 가요 .\n",
      "토큰 ID 시퀀스: [1, 4, 7, 4, 7, 4, 7, 4, 7, 8, 5]\n"
     ]
    }
   ],
   "source": [
    "text = \"12시 땡\"\n",
    "response = sentence_generation_beam(model, text, sp, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a288865-400e-4058-a31a-c3fc096a4fbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
